{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üß† INVESTIGACI√ìN: Aplicaci√≥n de Operaciones Matriciales en RNN para NLP\n",
    "\n",
    "**Tema:** Redes Neuronales Recurrentes en Procesamiento de Lenguaje Natural\n",
    "**Aplicaci√≥n:** Clasificaci√≥n de Sentimientos (Positivo/Negativo)\n",
    "\n",
    "Este cuaderno implementa un clasificador RNN simple **desde cero** utilizando `numpy` para:\n",
    "\n",
    "1.  Demostrar expl√≠citamente las **operaciones matriciales** fundamentales (multiplicaci√≥n de matrices) en el _forward_ y _backward_ pass.\n",
    "2.  Entrenar el modelo con un peque√±o conjunto de datos de rese√±as de pel√≠culas.\n",
    "3.  Analizar la **complejidad computacional** de estas operaciones.\n",
    "\n",
    "---\n",
    "**Arquitectura RNN Clave:**\n",
    "$$\\mathbf{h}_t = \\tanh(\\mathbf{x}_t \\mathbf{W}_{xh} + \\mathbf{h}_{t-1} \\mathbf{W}_{hh} + \\mathbf{b}_h)$$\n",
    "\n",
    "* $\\mathbf{x}_t \\mathbf{W}_{xh}$: Proyecci√≥n de la **entrada actual**.\n",
    "* $\\mathbf{h}_{t-1} \\mathbf{W}_{hh}$: Proyecci√≥n del **estado oculto anterior** (la recurrencia).\n",
    "* Ambas son **multiplicaciones matriciales** de alta densidad, clave para el rendimiento en hardware vectorizado (CPU/GPU). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import random\n",
    "from typing import Tuple, List\n",
    "import sys\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURACI√ìN GLOBAL\n",
    "# ============================================================================\n",
    "\n",
    "class Config:\n",
    "\t\"\"\"Par√°metros del modelo.\"\"\"\n",
    "\tVOCAB_SIZE = 200\n",
    "\tEMBEDDING_DIM = 32\n",
    "\tHIDDEN_DIM = 16\n",
    "\tOUTPUT_DIM = 2 # 0: Negativo, 1: Positivo\n",
    "\tLEARNING_RATE = 0.05 # Reducido un poco para mayor estabilidad en el ejemplo\n",
    "\tEPOCHS = 20 # Aumentado un poco para mejor convergencia\n",
    "\tBATCH_SIZE = 1\n",
    "\tMAX_SEQUENCE_LEN = 30\n",
    "\t\n",
    "\t# Visualizaci√≥n\n",
    "\tVERBOSE = True\n",
    "\tPRINT_EVERY = 5\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# CLASE PRINCIPAL: RNN IMPLEMENTACI√ìN MANUAL\n",
    "# ============================================================================\n",
    "\n",
    "class SimpleRNNClassifier:\n",
    "\t\"\"\"\n",
    "\tClasificador RNN manual que demuestra expl√≠citamente las operaciones\n",
    "\tmatriciales fundamentales en redes neuronales recurrentes.\n",
    "\t\"\"\"\n",
    "\t\n",
    "\tdef __init__(self, config: Config):\n",
    "\t\tself.config = config\n",
    "\t\tself.initialize_parameters()\n",
    "\t\t\n",
    "\tdef initialize_parameters(self):\n",
    "\t\t\"\"\"Inicializar matrices de peso con Xavier/He initialization (aproximaci√≥n).\"\"\"\n",
    "\t\t# Xavier initialization: var = 1 / n_in\n",
    "\t\txavier_embedding = np.sqrt(1.0 / self.config.EMBEDDING_DIM)\n",
    "\t\txavier_hidden = np.sqrt(1.0 / self.config.HIDDEN_DIM)\n",
    "\t\t\n",
    "\t\t# Matriz de Embedding (VOCAB_SIZE √ó EMBEDDING_DIM)\n",
    "\t\tself.W_embedding = np.random.randn(\n",
    "\t\t\tself.config.VOCAB_SIZE, \n",
    "\t\t\tself.config.EMBEDDING_DIM\n",
    "\t\t) * 0.01\n",
    "\t\t\n",
    "\t\t# Matriz entrada ‚Üí oculto (EMBEDDING_DIM √ó HIDDEN_DIM)\n",
    "\t\tself.W_xh = np.random.randn(\n",
    "\t\t\tself.config.EMBEDDING_DIM,\n",
    "\t\t\tself.config.HIDDEN_DIM\n",
    "\t\t) * xavier_embedding\n",
    "\t\t\n",
    "\t\t# Matriz oculto ‚Üí oculto (HIDDEN_DIM √ó HIDDEN_DIM)\n",
    "\t\tself.W_hh = np.random.randn(\n",
    "\t\t\tself.config.HIDDEN_DIM,\n",
    "\t\t\tself.config.HIDDEN_DIM\n",
    "\t\t) * xavier_hidden\n",
    "\t\t\n",
    "\t\t# Matriz oculto ‚Üí salida (HIDDEN_DIM √ó OUTPUT_DIM)\n",
    "\t\tself.W_hy = np.random.randn(\n",
    "\t\t\tself.config.HIDDEN_DIM,\n",
    "\t\t\tself.config.OUTPUT_DIM\n",
    "\t\t) * xavier_hidden\n",
    "\t\t\n",
    "\t\t# Sesgos\n",
    "\t\tself.b_h = np.zeros((1, self.config.HIDDEN_DIM))\n",
    "\t\tself.b_y = np.zeros((1, self.config.OUTPUT_DIM))\n",
    "\t\t\n",
    "\tdef forward(self, word_indices: List[int]) -> Tuple[np.ndarray, List, List]:\n",
    "\t\t\"\"\"\n",
    "\t\tForward pass (propagaci√≥n hacia adelante).\n",
    "\t\tOPERACIONES MATRICIALES CLAVE:\n",
    "\t\t1. x_t @ W_xh\n",
    "\t\t2. h_{t-1} @ W_hh\n",
    "\t\t3. h_t @ W_hy\n",
    "\t\t\"\"\"\n",
    "\t\th_t = np.zeros((1, self.config.HIDDEN_DIM))\n",
    "\t\th_sequence = [h_t.copy()]\n",
    "\t\tx_sequence = []\n",
    "\t\t\n",
    "\t\t# Procesar cada palabra en la secuencia (pasos temporales)\n",
    "\t\tfor word_idx in word_indices:\n",
    "\t\t\t# PASO 1: Embedding Lookup (similar a una multiplicaci√≥n matricial One-Hot @ W_embedding)\n",
    "\t\t\tx_t = self.W_embedding[word_idx:word_idx+1, :]\n",
    "\t\t\tx_sequence.append(x_t)\n",
    "\t\t\t\n",
    "\t\t\t# PASO 2: OPERACI√ìN MATRICIAL - Transformaci√≥n de entrada\n",
    "\t\t\t# (1 √ó embedding_dim) √ó (embedding_dim √ó hidden_dim) = (1 √ó hidden_dim)\n",
    "\t\t\th_input = x_t @ self.W_xh\n",
    "\t\t\t\n",
    "\t\t\t# PASO 3: OPERACI√ìN MATRICIAL - Transformaci√≥n recurrente (memoria)\n",
    "\t\t\t# (1 √ó hidden_dim) √ó (hidden_dim √ó hidden_dim) = (1 √ó hidden_dim)\n",
    "\t\t\th_recurrent = h_t @ self.W_hh\n",
    "\t\t\t\n",
    "\t\t\t# PASO 4: Combinaci√≥n lineal + activaci√≥n\n",
    "\t\t\tz = h_input + h_recurrent + self.b_h\n",
    "\t\t\th_t = np.tanh(z)\n",
    "\t\t\th_sequence.append(h_t.copy())\n",
    "\t\t\n",
    "\t\t# PASO 5: OPERACI√ìN MATRICIAL - Proyecci√≥n a espacio de salida (Clasificaci√≥n)\n",
    "\t\t# (1 √ó hidden_dim) √ó (hidden_dim √ó output_dim) = (1 √ó output_dim)\n",
    "\t\tlogits = h_t @ self.W_hy + self.b_y\n",
    "\t\t\n",
    "\t\treturn logits, h_sequence, x_sequence\n",
    "\t\n",
    "\tdef backward(self, word_indices: List[int], h_sequence: List,\n",
    "\t\t\t\t x_sequence: List, logits: np.ndarray, label: int):\n",
    "\t\t\"\"\"\n",
    "\t\tBackward pass: Backpropagation Through Time (BPTT).\n",
    "\t\tImplica multiplicaci√≥n matricial con la transpuesta (e.g., x.T @ delta) para calcular gradientes.\n",
    "\t\t\"\"\"\n",
    "\t\t# Error de predicci√≥n\n",
    "\t\tprobs = self._softmax(logits)\n",
    "\t\tdelta_y = probs.copy()\n",
    "\t\tdelta_y[0, label] -= 1 # Derivada de Cross-Entropy + Softmax\n",
    "\t\t\n",
    "\t\t# Gradientes de capa de salida\n",
    "\t\t# (hidden_dim √ó 1) √ó (1 √ó output_dim) = (hidden_dim √ó output_dim)\n",
    "\t\tdW_hy = h_sequence[-1].T @ delta_y \n",
    "\t\tdb_y = delta_y\n",
    "\t\t\n",
    "\t\t# Inicializar acumuladores de gradientes\n",
    "\t\tdW_xh = np.zeros_like(self.W_xh)\n",
    "\t\tdW_hh = np.zeros_like(self.W_hh)\n",
    "\t\tdb_h = np.zeros_like(self.b_h)\n",
    "\t\tdW_embedding = np.zeros_like(self.W_embedding)\n",
    "\t\t\n",
    "\t\t# Error retropropagado a capa oculta (delta_h)\n",
    "\t\tdelta_h = delta_y @ self.W_hy.T\n",
    "\t\t\n",
    "\t\t# Backprop a trav√©s de cada paso temporal (BPTT - tiempo invertido)\n",
    "\t\tfor t in range(len(word_indices) - 1, -1, -1):\n",
    "\t\t\tword_idx = word_indices[t]\n",
    "\t\t\t\n",
    "\t\t\t# Aplicar derivada de tanh\n",
    "\t\t\td_tanh = 1 - h_sequence[t+1] ** 2\n",
    "\t\t\tdelta_h_raw = delta_h * d_tanh # Element-wise multiplication\n",
    "\t\t\t\n",
    "\t\t\t# Gradientes de matrices (OPERACIONES MATRICIALES CLAVE: Transposici√≥n + Multiplicaci√≥n)\n",
    "\t\t\t# dW_xh: (emb_dim √ó 1) √ó (1 √ó hidden_dim) = (emb_dim √ó hidden_dim)\n",
    "\t\t\tdW_xh += x_sequence[t].T @ delta_h_raw \n",
    "\t\t\t# dW_hh: (hidden_dim √ó 1) √ó (1 √ó hidden_dim) = (hidden_dim √ó hidden_dim)\n",
    "\t\t\tdW_hh += h_sequence[t].T @ delta_h_raw \n",
    "\t\t\tdb_h += delta_h_raw\n",
    "\t\t\t\n",
    "\t\t\t# Gradiente del embedding\n",
    "\t\t\tdW_embedding[word_idx, :] += (delta_h_raw @ self.W_xh.T).flatten()\n",
    "\t\t\t\n",
    "\t\t\t# Propagar gradiente al paso anterior (recurrencia)\n",
    "\t\t\tdelta_h = delta_h_raw @ self.W_hh.T\n",
    "\t\t\t\n",
    "\t\t# Actualizar par√°metros (descenso de gradiente)\n",
    "\t\tself.W_xh -= self.config.LEARNING_RATE * dW_xh\n",
    "\t\tself.W_hh -= self.config.LEARNING_RATE * dW_hh\n",
    "\t\tself.W_hy -= self.config.LEARNING_RATE * dW_hy\n",
    "\t\tself.b_h -= self.config.LEARNING_RATE * db_h\n",
    "\t\tself.b_y -= self.config.LEARNING_RATE * db_y\n",
    "\t\tself.W_embedding -= self.config.LEARNING_RATE * dW_embedding\n",
    "\t\n",
    "\tdef predict(self, word_indices: List[int]) -> Tuple[int, float, np.ndarray]:\n",
    "\t\t\"\"\"Hacer predicci√≥n en una secuencia.\"\"\"\n",
    "\t\tlogits, _, _ = self.forward(word_indices)\n",
    "\t\tprobs = self._softmax(logits)\n",
    "\t\tpred_class = np.argmax(probs[0])\n",
    "\t\tconfidence = probs[0, pred_class]\n",
    "\t\treturn pred_class, confidence, probs[0]\n",
    "\t\n",
    "\t@staticmethod\n",
    "\tdef _softmax(x: np.ndarray) -> np.ndarray:\n",
    "\t\t\"\"\"Softmax estable num√©ricamente.\"\"\"\n",
    "\t\texp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "\t\treturn exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "\t\n",
    "\tdef calculate_loss(self, logits: np.ndarray, label: int) -> float:\n",
    "\t\t\"\"\"Calcular cross-entropy loss.\"\"\"\n",
    "\t\tprobs = self._softmax(logits)\n",
    "\t\treturn -np.log(probs[0, label] + 1e-10)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# UTILIDADES: PROCESAMIENTO DE TEXTO\n",
    "# ============================================================================\n",
    "\n",
    "class TextPreprocessor:\n",
    "\t\"\"\"Construir vocabulario y codificar texto.\"\"\"\n",
    "\t\n",
    "\tdef __init__(self, max_vocab: int = 500):\n",
    "\t\tself.max_vocab = max_vocab\n",
    "\t\tself.word2idx = {'<PAD>': 0, '<UNK>': 1}\n",
    "\t\tself.idx2word = {0: '<PAD>', 1: '<UNK>'}\n",
    "\t\tself.word_freq = defaultdict(int)\n",
    "\t\t\n",
    "\tdef build_vocab(self, texts: List[str]):\n",
    "\t\t\"\"\"Construir vocabulario a partir de textos.\"\"\"\n",
    "\t\tfor text in texts:\n",
    "\t\t\tfor word in text.lower().split():\n",
    "\t\t\t\tself.word_freq[word] += 1\n",
    "\t\t\n",
    "\t\t# A√±adir palabras m√°s frecuentes\n",
    "\t\tsorted_words = sorted(self.word_freq.items(), \n",
    "\t\t\t\t\t\t\t\t key=lambda x: x[1], reverse=True)\n",
    "\t\t\n",
    "\t\tfor idx, (word, freq) in enumerate(sorted_words[:self.max_vocab-2], 2):\n",
    "\t\t\tself.word2idx[word] = idx\n",
    "\t\t\tself.idx2word[idx] = word\n",
    "\t\n",
    "\tdef encode(self, text: str, max_len: int = None) -> List[int]:\n",
    "\t\t\"\"\"Convertir texto a √≠ndices.\"\"\"\n",
    "\t\tif max_len is None:\n",
    "\t\t\tmax_len = 30\n",
    "\t\t\n",
    "\t\twords = text.lower().split()[:max_len]\n",
    "\t\tindices = [self.word2idx.get(w, 1) for w in words] # 1 es <UNK>\n",
    "\t\t\n",
    "\t\t# Padding (relleno)\n",
    "\t\tif len(indices) < max_len:\n",
    "\t\t\tindices += [0] * (max_len - len(indices)) # 0 es <PAD>\n",
    "\t\t\n",
    "\t\treturn indices[:max_len]\n",
    "\t\n",
    "\tdef decode(self, indices: List[int]) -> str:\n",
    "\t\t\"\"\"Convertir √≠ndices a texto.\"\"\"\n",
    "\t\treturn ' '.join([self.idx2word.get(idx, '<UNK>') for idx in indices])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö Preparaci√≥n de Datos y Configuraci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. CONFIGURACI√ìN\n",
    "config = Config()\n",
    "print(\"üìä [1/5] CONFIGURACI√ìN DEL MODELO\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"  ‚úì Tama√±o vocabulario: {config.VOCAB_SIZE}\")\n",
    "print(f\"  ‚úì Dimensi√≥n embedding: {config.EMBEDDING_DIM}\")\n",
    "print(f\"  ‚úì Dimensi√≥n oculta: {config.HIDDEN_DIM}\")\n",
    "print(f\"  ‚úì Clases de salida: {config.OUTPUT_DIM}\")\n",
    "print(f\"  ‚úì √âpocas entrenamiento: {config.EPOCHS}\")\n",
    "print(f\"  ‚úì Tasa de aprendizaje: {config.LEARNING_RATE}\")\n",
    "print(\"\\n\")\n",
    "\n",
    "# 2. PREPARACI√ìN DE DATOS (Dataset de ejemplo)\n",
    "print(\"üìù [2/5] PREPARACI√ìN DE DATOS\")\n",
    "print(\"-\" * 80)\n",
    "positive_reviews = [\n",
    "\t\"esta pel√≠cula es absolutamente excelente muy buena\",\n",
    "\t\"obra maestra del cine incre√≠ble\",\n",
    "\t\"actores fant√°sticos historia fascinante\",\n",
    "\t\"mejor pel√≠cula del a√±o recomendada\",\n",
    "\t\"simplemente magn√≠fica y emocionante\",\n",
    "\t\"ador√© cada segundo pel√≠cula perfecta\",\n",
    "\t\"cinematograf√≠a hermosa actuaciones brillantes\",\n",
    "\t\"me encant√≥ totalmente genial\",\n",
    "]\n",
    "negative_reviews = [\n",
    "\t\"pel√≠cula horrible muy aburrida mala\",\n",
    "\t\"p√©rdida completa de tiempo\",\n",
    "\t\"actores terribles historia d√©bil\",\n",
    "\t\"muy mala no recomendada\",\n",
    "\t\"decepcionante y frustrante\",\n",
    "\t\"basura no aguant√© ver\",\n",
    "\t\"actuaciones malas gui√≥n p√©simo\",\n",
    "\t\"no entiendo por qu√© la hacen\",\n",
    "]\n",
    "\n",
    "all_texts = positive_reviews + negative_reviews\n",
    "all_labels = [1] * len(positive_reviews) + [0] * len(negative_reviews) # 1: Positivo, 0: Negativo\n",
    "\n",
    "print(f\"  ‚úì Total muestras: {len(all_texts)}\\n\")\n",
    "\n",
    "# 3. CONSTRUCCI√ìN DEL VOCABULARIO\n",
    "print(\"üî§ [3/5] CONSTRUCCI√ìN DEL VOCABULARIO\")\n",
    "print(\"-\" * 80)\n",
    "preprocessor = TextPreprocessor(max_vocab=config.VOCAB_SIZE)\n",
    "preprocessor.build_vocab(all_texts)\n",
    "\n",
    "print(f\"  ‚úì Palabras √∫nicas en dataset: {len(preprocessor.word2idx)}\\n\")\n",
    "\n",
    "# Codificar textos\n",
    "encoded_texts = [preprocessor.encode(text, max_len=config.MAX_SEQUENCE_LEN) \n",
    "\t\t\t\t\t\t for text in all_texts]\n",
    "\n",
    "print(f\"  ‚úì Ejemplo de codificaci√≥n (Primer Review Positivo):\")\n",
    "print(f\"  \\tTexto: \\\"{all_texts[0]}\\\"\")\n",
    "print(f\"  \\tCodificado: {encoded_texts[0][:15]}...\\n\")\n",
    "\n",
    "# 4. ARQUITECTURA DEL MODELO\n",
    "print(\"üß† [4/5] ARQUITECTURA DEL MODELO RNN\")\n",
    "print(\"-\" * 80)\n",
    "model = SimpleRNNClassifier(config)\n",
    "\n",
    "print(f\"  Matrices de Pesos Inicializadas:\")\n",
    "print(f\"  ‚îú‚îÄ W_embedding: \\t{model.W_embedding.shape} (Input: {config.VOCAB_SIZE}, Output: {config.EMBEDDING_DIM})\")\n",
    "print(f\"  ‚îú‚îÄ W_xh: \\t\\t{model.W_xh.shape} (Input: {config.EMBEDDING_DIM}, Output: {config.HIDDEN_DIM})\")\n",
    "print(f\"  ‚îú‚îÄ W_hh: \\t\\t{model.W_hh.shape} (Input: {config.HIDDEN_DIM}, Output: {config.HIDDEN_DIM})\")\n",
    "print(f\"  ‚îî‚îÄ W_hy: \\t\\t{model.W_hy.shape} (Input: {config.HIDDEN_DIM}, Output: {config.OUTPUT_DIM})\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéì Entrenamiento del Modelo (BPTT Manual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. ENTRENAMIENTO\n",
    "print(\"üéì [5/5] ENTRENAMIENTO DEL MODELO\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "training_losses = []\n",
    "training_accuracies = []\n",
    "total_samples = len(all_texts)\n",
    "\n",
    "for epoch in range(config.EPOCHS):\n",
    "\ttotal_loss = 0.0\n",
    "\tcorrect_predictions = 0\n",
    "\t\n",
    "\t# Shuffle data\n",
    "\tindices = list(range(total_samples))\n",
    "\trandom.shuffle(indices)\n",
    "\t\n",
    "\tfor idx in indices:\n",
    "\t\tword_indices = encoded_texts[idx]\n",
    "\t\tlabel = all_labels[idx]\n",
    "\t\t\n",
    "\t\t# Forward pass\n",
    "\t\tlogits, h_sequence, x_sequence = model.forward(word_indices)\n",
    "\t\t\n",
    "\t\t# Calcular loss\n",
    "\t\tloss = model.calculate_loss(logits, label)\n",
    "\t\ttotal_loss += loss\n",
    "\t\t\n",
    "\t\t# Predicci√≥n\n",
    "\t\tpred_class, _, _ = model.predict(word_indices)\n",
    "\t\tif pred_class == label:\n",
    "\t\t\tcorrect_predictions += 1\n",
    "\t\t\n",
    "\t\t# Backward pass (actualizar pesos)\n",
    "\t\tmodel.backward(word_indices, h_sequence, x_sequence, logits, label)\n",
    "\t\n",
    "\tavg_loss = total_loss / total_samples\n",
    "\taccuracy = correct_predictions / total_samples\n",
    "\t\n",
    "\ttraining_losses.append(avg_loss)\n",
    "\ttraining_accuracies.append(accuracy)\n",
    "\t\n",
    "\t# Imprimir progreso\n",
    "\tif (epoch + 1) % config.PRINT_EVERY == 0 or epoch == 0:\n",
    "\t\tprint(f\"  √âpoca {epoch+1:2d}/{config.EPOCHS} ‚îÇ \" +\n",
    "\t\t\t  f\"Loss: {avg_loss:.4f} ‚îÇ \" +\n",
    "\t\t\t  f\"Accuracy: {accuracy:5.1%}\")\n",
    "\n",
    "print(\"-\" * 80)\n",
    "print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úÖ Pruebas de Predicci√≥n\n",
    "\n",
    "Probamos el modelo entrenado con frases no vistas en el conjunto de entrenamiento para evaluar su capacidad de **generalizaci√≥n**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"‚úÖ PRUEBAS DE PREDICCI√ìN\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "test_cases = [\n",
    "\t(\"pel√≠cula excelente muy buena recomendada\", 1),\n",
    "\t(\"horrible mala aburrida p√©rdida de tiempo\", 0),\n",
    "\t(\"actores incre√≠bles historia fascinante\", 1),\n",
    "\t(\"decepcionante y frustrante\", 0),\n",
    "\t(\"me encant√≥ totalmente magn√≠fica\", 1),\n",
    "\t(\"basura no aguant√©\", 0),\n",
    "]\n",
    "\n",
    "correct_test = 0\n",
    "\n",
    "for test_idx, (test_text, true_label) in enumerate(test_cases, 1):\n",
    "\tword_indices = preprocessor.encode(test_text, max_len=config.MAX_SEQUENCE_LEN)\n",
    "\tpred_label, confidence, probs = model.predict(word_indices)\n",
    "\t\n",
    "\t# Verificar si es correcto\n",
    "\tis_correct = pred_label == true_label\n",
    "\tif is_correct:\n",
    "\t\tcorrect_test += 1\n",
    "\t\n",
    "\t# Mostrar resultado\n",
    "\tsymbol = \"‚úì\" if is_correct else \"‚úó\"\n",
    "\tsentiment = \"POSITIVO ‚ú®\" if pred_label == 1 else \"NEGATIVO ‚ùå\"\n",
    "\t\n",
    "\tprint(f\"\\n  {symbol} Test {test_idx}: '{test_text}'\")\n",
    "\tprint(f\"  \\tPredicci√≥n: {sentiment}\")\n",
    "\tprint(f\"  \\tConfianza: {confidence*100:.1f}%\")\n",
    "\tprint(f\"  \\tP(Neg): {probs[0]:.4f} | P(Pos): {probs[1]:.4f}\")\n",
    "\n",
    "test_accuracy = correct_test / len(test_cases)\n",
    "print(f\"\\n  Precisi√≥n en pruebas: {correct_test}/{len(test_cases)} ({test_accuracy*100:.1f}%)\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üî¨ An√°lisis Detallado de Operaciones Matriciales\n",
    "\n",
    "La eficiencia de una RNN se mide por el n√∫mero de **multiplicaciones matriciales** (MACs - Multiply-Accumulate Operations) que realiza, especialmente en las capas ocultas, que se repiten para cada palabra de la secuencia. La aceleraci√≥n por **GPU/TPU** se basa en la capacidad de paralelizar estas operaciones.\n",
    "\n",
    "El costo de c√°lculo de la matriz $\\mathbf{A} \\mathbf{B}$ de tama√±o $(m \\times k) \\times (k \\times n)$ es $O(mkn)$. En nuestro caso, $m=1$ (tama√±o de *batch*), por lo que el costo es $O(kn)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. AN√ÅLISIS DE OPERACIONES MATRICIALES\n",
    "print(\"üî¨ AN√ÅLISIS DETALLADO DE OPERACIONES MATRICIALES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "max_seq = config.MAX_SEQUENCE_LEN\n",
    "emb_dim = config.EMBEDDING_DIM\n",
    "hidden_dim = config.HIDDEN_DIM\n",
    "num_samples = len(all_texts)\n",
    "num_epochs = config.EPOCHS\n",
    "\n",
    "# Operaciones por paso temporal (Forward Pass)\n",
    "ops_xh = emb_dim * hidden_dim # x_t @ W_xh\n",
    "ops_hh = hidden_dim * hidden_dim # h_t @ W_hh\n",
    "ops_per_step = ops_xh + ops_hh\n",
    "ops_per_seq = max_seq * ops_per_step\n",
    "\n",
    "print(f\"\\n  Operaciones por paso temporal (Solo Forward):\")\n",
    "print(f\"  ‚îú‚îÄ x_t @ W_xh: {emb_dim} √ó {hidden_dim} = {ops_xh:,} mults\")\n",
    "print(f\"  ‚îú‚îÄ h_t @ W_hh: {hidden_dim} √ó {hidden_dim} = {ops_hh:,} mults\")\n",
    "print(f\"  ‚îî‚îÄ Total por paso: {ops_per_step:,} mults\")\n",
    "\n",
    "print(f\"\\n  Operaciones por secuencia ({max_seq} palabras):\")\n",
    "print(f\"  ‚îî‚îÄ {max_seq} pasos √ó {ops_per_step:,} ops/paso = {ops_per_seq:,} ops\")\n",
    "\n",
    "# Operaciones totales de entrenamiento (Forward + Backward)\n",
    "# El Backward es aproximadamente 2-3 veces m√°s costoso que el Forward\n",
    "ops_scaling_factor = 3 \n",
    "total_ops_training = num_samples * num_epochs * ops_per_seq * ops_scaling_factor\n",
    "\n",
    "print(f\"\\n  Operaciones totales (entrenamiento estimado, incluyendo BPTT):\")\n",
    "print(f\"  ‚îî‚îÄ {num_samples} muestras √ó {num_epochs} √©pocas √ó {ops_per_seq:,} ops/seq √ó {ops_scaling_factor}x (BPTT) = {total_ops_training:,} ops\")\n",
    "print(f\"  ‚îî‚îÄ ‚âà {total_ops_training / 1e6:.2f}M multiplicaciones/sumas (MACs)\")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìê Demostraci√≥n Expl√≠cita de Multiplicaci√≥n Matricial\n",
    "\n",
    "El n√∫cleo de la recurrencia es la suma de dos proyecciones lineales, donde $\\mathbf{x}_t$ y $\\mathbf{h}_{t-1}$ se convierten en el nuevo estado $\\mathbf{h}_t$. Esta es la operaci√≥n m√°s costosa y fundamental."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìê DEMOSTRACI√ìN EXPL√çCITA DE OPERACI√ìN MATRICIAL\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Crear matrices peque√±as para visualizaci√≥n\n",
    "emb_dim = 4\n",
    "hidden_dim = 3\n",
    "\n",
    "x_demo = np.array([[1.0, 0.5, -0.3, 0.2]]) # Embedding (1 √ó 4)\n",
    "h_prev_demo = np.array([[0.1, -0.2, 0.3]]) # Estado anterior (1 √ó 3)\n",
    "W_xh_demo = np.random.randn(emb_dim, hidden_dim) * 0.1 # (4 √ó 3)\n",
    "W_hh_demo = np.random.randn(hidden_dim, hidden_dim) * 0.1 # (3 √ó 3)\n",
    "\n",
    "print(f\"\\n  1Ô∏è‚É£  ENTRADA (x_t): Shape {x_demo.shape}\")\n",
    "print(f\"  2Ô∏è‚É£  ESTADO ANTERIOR (h_{{t-1}}): Shape {h_prev_demo.shape}\")\n",
    "print(f\"  3Ô∏è‚É£  MATRIZ W_xh (Entrada‚ÜíOculto): Shape {W_xh_demo.shape}\")\n",
    "print(f\"  4Ô∏è‚É£  MATRIZ W_hh (Oculto‚ÜíOculto): Shape {W_hh_demo.shape}\")\n",
    "\n",
    "# Operaciones\n",
    "z1 = x_demo @ W_xh_demo\n",
    "z2 = h_prev_demo @ W_hh_demo\n",
    "z = z1 + z2 # Se omite b_h para simplificar\n",
    "h_new = np.tanh(z)\n",
    "\n",
    "print(f\"\\n  ‚úñÔ∏è  OPERACI√ìN 1: x_t @ W_xh\")\n",
    "print(f\"      ({x_demo.shape}) √ó ({W_xh_demo.shape}) = ({z1.shape})\")\n",
    "print(f\"      Resultado (Proyecci√≥n de entrada): {np.round(z1, 4)}\")\n",
    "\n",
    "print(f\"\\n  ‚úñÔ∏è  OPERACI√ìN 2: h_prev @ W_hh\")\n",
    "print(f\"      ({h_prev_demo.shape}) √ó ({W_hh_demo.shape}) = ({z2.shape})\")\n",
    "print(f\"      Resultado (Proyecci√≥n de estado anterior): {np.round(z2, 4)}\")\n",
    "\n",
    "print(f\"\\n  ‚ûï COMBINACI√ìN Y ACTIVACI√ìN: h_t = tanh(z1 + z2)\")\n",
    "print(f\"      NUEVO ESTADO OCULTO (h_t): {np.round(h_new, 4)}\")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìå Conclusiones\n",
    "\n",
    "1.  **Multiplicaci√≥n Matricial es la Base:** La recurrencia y la transformaci√≥n de la entrada en cada paso temporal dependen de una o m√°s multiplicaciones matriciales. La eficiencia de un algoritmo RNN (y de todos los modelos modernos de Deep Learning) est√° directamente ligada a la optimizaci√≥n del hardware para estas operaciones.\n",
    "\n",
    "2.  **Backpropagation Through Time (BPTT):** El entrenamiento requiere propagar el error hacia atr√°s no solo a trav√©s de las capas, sino tambi√©n a trav√©s del tiempo, lo que implica a√∫n m√°s multiplicaciones matriciales (e.g., $h_{t-1}.T \\mathbf{d}h_t$ para $\\mathbf{W}_{hh}$).\n",
    "\n",
    "3.  **Importancia del Hardware:** La estimaci√≥n de que una GPU puede ser $\\approx 50,000\\times$ m√°s r√°pida que una CPU en este tipo de carga ilustra por qu√© la **aceleraci√≥n de matrices** es fundamental para el desarrollo de NLP a escala real."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}